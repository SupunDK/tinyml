{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqfmB51r7ps6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, MaxPooling1D, Conv1D, SpatialDropout1D, Flatten, GlobalAveragePooling1D, Dropout, AveragePooling1D, Input, Concatenate, Activation, Add, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "import os\n",
        "!pip install tf2onnx\n",
        "import tf2onnx\n",
        "import onnx\n",
        "import numpy as np\n",
        "!pip install tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "!pip install onnxruntime\n",
        "import onnxruntime as ort\n",
        "!pip install wandb\n",
        "!wandb login 676572de01bb78bb4574de14deed1133349c0591\n",
        "import wandb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "!pip install keras-swa\n",
        "from swa.tfkeras import SWA\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "ESl_MSTzP3_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to TPU\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EizoqAdxMpXY",
        "outputId": "8a5530a3-5796-421a-96dd-526ca3f1627a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.12.0\n",
            "Running on TPU  ['10.76.124.26:8470']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the test and train dataset as TFRecords\n",
        "def parse(serialized,signal_shape=(1250,1),label_shape=(1,)):\n",
        "\n",
        "    features = {'signal': tf.io.FixedLenFeature([], tf.string), 'label': tf.io.FixedLenFeature([], tf.int64)}\n",
        "    parsed_example = tf.io.parse_single_example(serialized=serialized, features=features)\n",
        "\n",
        "    signal = parsed_example['signal']\n",
        "    label = parsed_example['label']\n",
        "\n",
        "    signal = tf.io.decode_raw(signal, tf.float64)\n",
        "    signal = tf.reshape(signal, shape=signal_shape)\n",
        "\n",
        "    label = tf.reshape(label, shape=label_shape)\n",
        "\n",
        "    return signal, label\n",
        "\n",
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "train_files = tf.io.matching_files('gs://tinyml/train/*train.tfrec')\n",
        "train_files = tf.random.shuffle(train_files)\n",
        "shards = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "train_dataset = shards.interleave(lambda x: tf.data.TFRecordDataset(x), num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
        "train_dataset = train_dataset.map(parse, num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(128)\n",
        "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "test_files = tf.io.matching_files('gs://tinyml/test/*test.tfrec')\n",
        "test_files = tf.random.shuffle(test_files)\n",
        "shards = tf.data.Dataset.from_tensor_slices(test_files)\n",
        "test_dataset = shards.interleave(lambda x: tf.data.TFRecordDataset(x), num_parallel_calls=AUTOTUNE)\n",
        "test_dataset = test_dataset.map(parse, num_parallel_calls=AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(128)\n",
        "test_dataset = test_dataset.prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "vNBj4X9MT-4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(learning_rate):\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(1250, 1)),\n",
        "    keras.layers.Conv1D(filters=3, kernel_size=85, strides=32, padding='valid', activation=None, use_bias=True),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(20),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Dropout(0.1),\n",
        "    keras.layers.Dense(10),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Dense(2),\n",
        "  ])\n",
        "\n",
        "  # compiling the model\n",
        "  AUROC = tf.keras.metrics.AUC(curve='ROC', name = 'AUROC',multi_label = True)\n",
        "  AUPRC = tf.keras.metrics.AUC(curve='PR', name = 'AUPRC',multi_label = True)\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy', AUROC, AUPRC,tfa.metrics.FBetaScore(average='micro',num_classes=2,beta = 2.0, threshold=0.5)]\n",
        "            )\n",
        "  tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=False,show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96,layer_range=None, show_layer_activations=False)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Dcg0EcdsPfJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_once(count):\n",
        "  # hyperparameters\n",
        "  learning_rate = 0.0002\n",
        "  num_epochs = 100\n",
        "  batch_size = 32\n",
        "\n",
        "  # data augmentation settings\n",
        "  data_aug = True # master flag that enabled the following data augmentations\n",
        "  mix = False # mixing input and output data\n",
        "  flip_peak = True # changing the signs of the values in the data (multiplying the data by -1)\n",
        "  flip_time = False # reversing the signal\n",
        "  add_noise = True # adding noise to the signal\n",
        "\n",
        "  # Stochastic Weight Averaging\n",
        "  start_epoch = 10\n",
        "  swa = SWA(start_epoch=start_epoch,\n",
        "        lr_schedule='cyclic',\n",
        "        swa_lr=0.0001,\n",
        "        swa_lr2=0.0005,\n",
        "        swa_freq=5,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1)\n",
        "\n",
        "  with tpu_strategy.scope():\n",
        "      my_model = create_model(learning_rate)\n",
        "\n",
        "  save_name = 'random_' + str(count)\n",
        "  # save_name = 'SWA'\n",
        "  checkpoint_filepath = './20_10/' + save_name + '/'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_filepath,\n",
        "      save_weights_only=True,\n",
        "      monitor='val_accuracy',\n",
        "      mode='max',\n",
        "      save_best_only=True)\n",
        "\n",
        "  # training the model\n",
        "  my_model.fit(train_dataset, validation_data=test_dataset, epochs=num_epochs, batch_size=batch_size, shuffle=True, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "  # evaluating the trained model\n",
        "  my_model.load_weights(checkpoint_filepath)\n",
        "  orig_loss,orig_accuracy,orig_AUROC,orig_AUPRC,orig_fbeta_score = model.evaluate(test_dataset)\n",
        "  print('Model: ', save_name)\n",
        "  print(f\"loss: {orig_loss} - accuracy: {orig_accuracy} - AUROC: {orig_AUROC} - AUPRC: {orig_AUPRC} - fbeta_score: {orig_fbeta_score}\")\n",
        "\n",
        "  return my_model"
      ],
      "metadata": {
        "id": "Qt0qDYc8RfVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = run_once(1)"
      ],
      "metadata": {
        "id": "YnUAsVGqg_XJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}